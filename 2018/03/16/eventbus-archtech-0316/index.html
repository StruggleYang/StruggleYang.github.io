<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="StruggleYang  struy"><title>事件总线EventBus实现架构原理分析 | struy</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">事件总线EventBus实现架构原理分析</h1><a id="logo" href="/.">struy</a><p class="description">秋天该很好，你若尚在场！</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/soul/"><i class="fa fa-wifi"> 午后</i></a><a href="/tools/"><i class="fa fa-wrench"> 红茶</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a><a href="/collect"><i class="fa fa-rss"> collect</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">事件总线EventBus实现架构原理分析</h1><div class="post-meta">Mar 16, 2018</div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#DDD领域事件架构简析"><span class="toc-number">1.</span> <span class="toc-text">DDD领域事件架构简析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#跨领域事件消息保障"><span class="toc-number">2.</span> <span class="toc-text">跨领域事件消息保障</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka对消息发送的保障"><span class="toc-number">2.1.</span> <span class="toc-text">kafka对消息发送的保障</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka的三种语义"><span class="toc-number">2.1.1.</span> <span class="toc-text">Kafka的三种语义</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#生产者端（Producer）消息保障"><span class="toc-number">2.2.</span> <span class="toc-text">生产者端（Producer）消息保障</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#消费者端（Consumer）消息保障"><span class="toc-number">2.3.</span> <span class="toc-text">消费者端（Consumer）消息保障</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#EventBus（事件总线）收发消息具体实现"><span class="toc-number">3.</span> <span class="toc-text">EventBus（事件总线）收发消息具体实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#EventBus生产端（Producer）"><span class="toc-number">3.1.</span> <span class="toc-text">EventBus生产端（Producer）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#实现生产者端至少一次"><span class="toc-number">3.1.1.</span> <span class="toc-text">实现生产者端至少一次</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#生产者配置"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">生产者配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#创建事务生产者"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">创建事务生产者</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#批量消息发送"><span class="toc-number">3.1.1.3.</span> <span class="toc-text">批量消息发送</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#消息轮询"><span class="toc-number">3.1.2.</span> <span class="toc-text">消息轮询</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#设计目的"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">设计目的</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#生产端总结"><span class="toc-number">3.1.3.</span> <span class="toc-text">生产端总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Eventbus消息端（Consumer）"><span class="toc-number">3.2.</span> <span class="toc-text">Eventbus消息端（Consumer）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#消息的编解码"><span class="toc-number">3.2.1.</span> <span class="toc-text">消息的编解码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实现原理"><span class="toc-number">3.2.2.</span> <span class="toc-text">实现原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#消费端注意事项"><span class="toc-number">3.2.3.</span> <span class="toc-text">消费端注意事项</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#总结"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><blockquote>
<p>事件总线(EventBus)设计初衷是解耦系统模块，将系统中的各类业务操作抽象为事件模型。我们把产生事件的部分称之为事件的发送者(Publisher)，消费事件的部分称之为订阅者(Subcriber)。</p>
</blockquote>
<a id="more"></a>
<p><a href="https://github.com/dapeng-soa" target="_blank" rel="noopener">@大鹏开源</a>:别看我有点萌，我可以秒变大鹏。<br><img src="http://upload-images.jianshu.io/upload_images/6393906-b7ba572342b3e4ba..jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="大鹏鸟"></p>
<h1 id="DDD领域事件架构简析"><a href="#DDD领域事件架构简析" class="headerlink" title="DDD领域事件架构简析"></a>DDD领域事件架构简析</h1><p>领域事件是领域驱动设计(Domain Driven Design，简称DDD)中的一个概念，领域模型的变化会产生领域事件，事件的产生往往伴随着相应的动作。例如，用户在完成注册后，系统会发出一封带有确认信息的邮件到用户的邮箱。有了领域事件，每个领域本身就只需要关系其自己的业务逻辑，并在处理完自身逻辑的同时抛出相应的领域事件。对这些领域事件感兴趣的业务方可以订阅该事件，然后进行后续的处理。</p>
<p>结合领域事件和EventBus，我们来看一下基于Today中台架构的EventBus架构模型：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/6393906-a0cdb1c3e8040895.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="eventbus模型.png"></p>
<p>领域事件产生后会有监听者进行事件监听。在如此众多的事件中，需要一个消息总线对消息进行统一的管理。消息总线的作用有：</p>
<p> 1.一个大的领域内的各小模块之间的事件的触发和订阅；</p>
<p> 2.跨领域（跨JVM进程）之间的事件订阅。</p>
<p>首先领域内某个方法触发事件，例如<code>registerUser()</code>方法执行成功后，会发布一个用户创建的事件。这时候，我们只需要调用<code>EventBus.fireEvent()</code>方法，就会将事件发布出去，在EventBus内部，如果本领域内有订阅者订阅此事件，Eventbus就会匹配消息的类型，然后进行相应事件的逻辑处理，这样就做到了领域内的事件监听和处理。</p>
<p>在这种情况下，事件的发布和订阅是强一致性的，两者在同一个处理过程中，当出现错误，触发事件方和订阅方的数据会一起回滚。</p>
<p>不管是领域内事件还是跨领域事件，在EventBus触发事件之后，都会选择将消息持久化到业务数据库。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def fireEvent(event: Any): Unit = &#123;</span><br><span class="line">  // 本地事件分发处理</span><br><span class="line">  dispatchEvent(event)</span><br><span class="line"> //持久化事件</span><br><span class="line">  persistenceEvent(event)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="跨领域事件消息保障"><a href="#跨领域事件消息保障" class="headerlink" title="跨领域事件消息保障"></a>跨领域事件消息保障</h1><p>跨领域消息需要通过消息中间件(MQ)来进行传递，目的就是解除系统间的强依赖性，解耦业务模块。</p>
<p>我们的事件总线发布的跨领域（系统）消息，是通过kafka这款消息中间件来进行存储转发的。因为每一条事件消息对业务系统都十分重要，我们要确保发送消息不能丢失，并最终能够被订阅者订阅到。</p>
<h2 id="kafka对消息发送的保障"><a href="#kafka对消息发送的保障" class="headerlink" title="kafka对消息发送的保障"></a>kafka对消息发送的保障</h2><p>Kafka的语义很直接，当发布一条消息时，<code>kafka broker</code>收到后并 <code>committed</code>到了日志，那么在<code>broker</code>看来, 这个消息已经发布成功了，同时会有一个ack消息返回给生产者。<br></p>
<p>当网络不可靠时, 我们会碰到这样一种情形：生产者发送消息时发生网络错误，这时将无法确定错误发生在broker收到消息前还是在收到消息后(返回ack丢失)。</p>
<h3 id="Kafka的三种语义"><a href="#Kafka的三种语义" class="headerlink" title="Kafka的三种语义"></a>Kafka的三种语义</h3><ul>
<li>At most once  <br><br>最多一次：消息可能丢失，但绝不会重发。</li>
<li>At least once <br><br>至少一次：消息绝不会丢失，但有可能重新发送。</li>
</ul>
<ul>
<li>Exactly once  <br><br>正好一次：这是我们真正想要的，每个消息传递一次且仅一次。</li>
</ul>
<p>这里我们结合上述三种语义并以发送端（生产者）和接收端（消费者）对消息消费语义的保障分别进行探讨。可分解成两个问题：发送消息时的持久性保障和消费消息的保障。</p>
<h2 id="生产者端（Producer）消息保障"><a href="#生产者端（Producer）消息保障" class="headerlink" title="生产者端（Producer）消息保障"></a>生产者端（Producer）消息保障</h2><ul>
<li>最多一次：业务系统创建kafka生产者实例时，配置消息发送重试次数为0，如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(retries, 0);</span><br></pre></td></tr></table></figure>
<p>当消息发送失败时，不会再尝试进行消息发送，这样虽然保证了消息不会重复发送，但是有可能会丢失消息，这是系统不能容忍的。</p>
<ul>
<li><p>至少一次：创建kafka生产者实例时，配置重试次数大于等于1，如下配置就是重试2次，如果生产者连续发送消息成功而没有收到ack，就会重试两次。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(retries, 2);</span><br></pre></td></tr></table></figure>
</li>
<li><p>正好一次：在kafka新版中，增加了事务消息发送功能并支持发送消息的幂等性配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(enable.idempotence, &quot;true&quot;);</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这样kafka在至少一次语义的基础上，同时保证重复发送的消息不会导致日志重复，完美做到了发送端正好一次。</p>
<p>它的原理就是<code>kafka-broker</code>为每一个生产者分配了一个<code>PID</code>，以便区分不同的生产者实例，当然这个<code>PID</code>对用户不透明，用户无需进行配置。开启了消息幂等后，producer在发送每条消息时，都会带上一个序列号(sequnceId)，这样可以在重复发送时根据序列号进行去重。</p>
<h2 id="消费者端（Consumer）消息保障"><a href="#消费者端（Consumer）消息保障" class="headerlink" title="消费者端（Consumer）消息保障"></a>消费者端（Consumer）消息保障</h2><ul>
<li>消费者接收到broker的消息后，会提交偏移量到broker上来表示消息消费成功。</li>
<li>我们将消费者配置为手动提交偏移量，在整个消费消息逻辑处理完成，并没有抛出异常后，手动提交偏移量。</li>
<li>如果偏移量未成功提交，消息将会再次发送给消费者端，这样就能够保障消息不会丢失。</li>
</ul>
<hr>
<h1 id="EventBus（事件总线）收发消息具体实现"><a href="#EventBus（事件总线）收发消息具体实现" class="headerlink" title="EventBus（事件总线）收发消息具体实现"></a>EventBus（事件总线）收发消息具体实现</h1><h2 id="EventBus生产端（Producer）"><a href="#EventBus生产端（Producer）" class="headerlink" title="EventBus生产端（Producer）"></a>EventBus生产端（Producer）</h2><h3 id="实现生产者端至少一次"><a href="#实现生产者端至少一次" class="headerlink" title="实现生产者端至少一次"></a>实现生产者端至少一次</h3><p>我们也会考虑这样一种情况，当业务系统需要进行批量消息发送时，如果中途某条消息发送失败后，需要之前发送成功的消息一起回滚，类似于数据库上的事务回滚。</p>
<h4 id="生产者配置"><a href="#生产者配置" class="headerlink" title="生产者配置"></a>生产者配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">properties.put(&quot;transactional.id&quot;,&quot;GOODS_1213&quot;);</span><br><span class="line">properties.put(&quot;enable.idempotence&quot;, &quot;true&quot;);</span><br></pre></td></tr></table></figure>
<ul>
<li>首先kafka对事务消息的支持前提是，需要为每个生产者实例显式的配置唯一的事务id号（如上配置），</li>
<li>其次需要将kafka消息幂等功能设置为true。</li>
</ul>
<p>通过这样的配置之后，消息在批量进行发送出现异常时，放弃当前事务即可。<br>我们实现的<code>eventbus</code>事务发送端代码如下:</p>
<h4 id="创建事务生产者"><a href="#创建事务生产者" class="headerlink" title="创建事务生产者"></a>创建事务生产者</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private def initTransProducer(transId: String): Unit = &#123;</span><br><span class="line">    val builder = KafkaConfigBuilder.defaultProducer</span><br><span class="line">    val properties = builder</span><br><span class="line">      .withKeySerializer(classOf[LongSerializer])</span><br><span class="line">      .withValueSerializer(classOf[ByteArraySerializer])</span><br><span class="line">      .bootstrapServers(serverHost)</span><br><span class="line">      .withTransactions(transId)</span><br><span class="line">      .withIdempotence(&quot;true&quot;) //幂等性保证</span><br><span class="line">      .build</span><br><span class="line"></span><br><span class="line">    producer = new KafkaProducer[Long, Array[Byte]](properties)</span><br><span class="line">    //初始化事务</span><br><span class="line">    producer.initTransactions()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="批量消息发送"><a href="#批量消息发送" class="headerlink" title="批量消息发送"></a>批量消息发送</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def batchSend(topic: String, eventMessage: util.List[EventStore]): Unit = &#123;</span><br><span class="line">try &#123;</span><br><span class="line">      producer.beginTransaction()</span><br><span class="line">      eventMessage.forEach((eventStore: EventStore) =&gt; &#123;</span><br><span class="line">        producer.send(new ProducerRecord[Long, Array[Byte]](topic, eventStore.id, eventStore.eventBinary), (metadata: RecordMetadata, exception: Exception) =&gt; &#123;</span><br><span class="line">          logger.info(</span><br><span class="line">            s&quot;&quot;&quot;in transaction per msg ,send message to broker successful,</span><br><span class="line">        id: $&#123;eventStore.id&#125;, topic: $&#123;metadata.topic&#125;, offset: $&#123;metadata.offset&#125;, partition: $&#123;metadata.partition&#125;&quot;&quot;&quot;)</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125;)</span><br><span class="line">      //没有异常，提交事务</span><br><span class="line">producer.commitTransaction()</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt;</span><br><span class="line">      //回滚事务</span><br><span class="line">        producer.abortTransaction()</span><br><span class="line">        logger.error(e.getMessage, e)</span><br><span class="line">        logger.error(&quot;send message failed,topic: &#123;&#125;&quot;, topic)</span><br><span class="line">        throw e</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>上面代码能够保障在批量发送消息时的事务一致性：要么消息全部发送成功，提交事务；如果出现一条失败，则事务回滚。</p>
<p>当然，为了配合事务消息，消费端需要在创建kafka消费者实例时，配置消息获取的隔离级别为<code>read_committed</code>（读取已经提交的消息）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(&quot;isolation.level&quot;, &quot;read_committed&quot;);</span><br></pre></td></tr></table></figure></p>
<p>这样既可以保障事务消息的发送，又能保障幂等性，在消息发送端不考虑非常极端的情况下，是已经做到了<strong>正好一次</strong>的语义,考虑极端情况下，也可以做到至少一次。</p>
<h3 id="消息轮询"><a href="#消息轮询" class="headerlink" title="消息轮询"></a>消息轮询</h3><blockquote>
<p>业务系统触发事件之后，会将事件存储在数据库中，然后会有一个定时任务线程定时去轮询数据库获取消息，并使用kafka将消息发送到broker。</p>
</blockquote>
<h4 id="设计目的"><a href="#设计目的" class="headerlink" title="设计目的"></a>设计目的</h4><p>使用数据库的事务来保障业务服务接口产生的事件跟业务行为保持强一致性，而在定时发送消息时，也会使用数据库事务+kafka的事务，将消息发送出去。以下为发送消息的逻辑：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def doPublishMessages(): Unit = &#123;</span><br><span class="line">    // 消息总条数计数器</span><br><span class="line">    val counter = new AtomicInteger(0)</span><br><span class="line">    // 批量处理, 每次从数据库取出消息的最大数量(window)</span><br><span class="line">    val window = 100</span><br><span class="line">    // 单轮处理的消息计数器, 用于控制循环退出.</span><br><span class="line">    val resultSetCounter = new AtomicInteger(window)</span><br><span class="line">    //当一次轮询拿到的消息达到了最大数时，会再次轮询数据库，以确保还有消息没有被发送</span><br><span class="line">    while (resultSetCounter.get() == window) &#123;</span><br><span class="line">      resultSetCounter.set(0)</span><br><span class="line">      //数据库事务开启</span><br><span class="line">      dataSource.withTransaction[Unit](conn =&gt; &#123;</span><br><span class="line">        //AAA</span><br><span class="line">        conn.eachRow[EventStore](sql&quot;SELECT * FROM common_event limit $&#123;window&#125; FOR UPDATE&quot;)(event =&gt; &#123;</span><br><span class="line">          conn.executeUpdate(sql&quot;DELETE FROM common_event WHERE id = $&#123;event.id&#125;&quot;)</span><br><span class="line">          //Kafka事务确保send失败会回滚并抛异常</span><br><span class="line">          producer.send(topic, event.id, event.eventBinary)</span><br><span class="line">          resultSetCounter.incrementAndGet()</span><br><span class="line">          counter.incrementAndGet()</span><br><span class="line">        //AAA</span><br><span class="line">           </span><br><span class="line">        &#125;)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在上面AAA块的代码中，我们将消息获取和kafka生产者发送消息放到了一个事务里，首先获取到消息，然后删除当前消息，再将消息发送到kafka上，保障消息刚好一次的发送。<br></p>
<p>如果这时候出现异常(kafka发送数据异常或者删除数据库记录异常)，事务就会回滚，消息没有被真正删除，下一次轮询会再次执行这个过程。不考虑极端的情况下，这样的做法可以保证正好一次的消息发送。<br></p>
<p>考虑极端情况，即消息的删除和kafka发送都成功了，而在事务提交那行代码时，出现异常。这时候，数据库会回滚，但是消息已经被发送到了kafka上，因此这里还是会出现消息的重复发送，但是做到了至少一次, 而且这种情况是极其极端的。</p>
<h3 id="生产端总结"><a href="#生产端总结" class="headerlink" title="生产端总结"></a>生产端总结</h3><p>我们已经在不考虑极端的情况下（服务器宕机）做到了正好一次，考虑极端的情况下，做到了至少一次。<br></p>
<p>然而，极端情况还是有可能会出现，因此最好的解决方案是，我们还会在事件消息上面标记一个事件唯一ID，以支持用户在业务系统上消费消息的时候做幂等处理。<br></p>
<p>如下是一个标准的业务事件的定义。（详情参考杨权文章）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">PublishedSkuEvent</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">   //事件唯一id</span></span></span><br><span class="line"><span class="class"><span class="params">   id : <span class="type">Long</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">   //事件中携带的具体业务</span></span></span><br><span class="line"><span class="class"><span class="params">   skuIds : <span class="type">List</span>[<span class="type">Long</span>]</span></span></span><br><span class="line"><span class="class"><span class="params"> </span>)</span></span><br></pre></td></tr></table></figure>
<p>当然并不是所有的情况都需要这么强力的保障，比如对重复消息不是很敏感的情况，我们不需要发送事务消息或保证消息幂等性，只需要做到消息不会丢失即可。对于延迟敏感的，我们允许生产者指定它想要的耐用性水平。如生产者可以指定它获取需等待10毫秒量级上的响应。生产者也可以指定异步发送，或只等待leader（不需要副本的响应）有响应。</p>
<h2 id="Eventbus消息端（Consumer）"><a href="#Eventbus消息端（Consumer）" class="headerlink" title="Eventbus消息端（Consumer）"></a>Eventbus消息端（Consumer）</h2><blockquote>
<p>消费端分为领域内事件消费和跨领域事件消费，领域内由于在同一个事务内，强一致性，实现简单，我们这里不再赘述。现在，我们来分析下如何做到跨领域消息订阅和接收。</p>
</blockquote>
<h3 id="消息的编解码"><a href="#消息的编解码" class="headerlink" title="消息的编解码"></a>消息的编解码</h3><p>前文没有提到的一点，我们将事件定义为thrift结构体，并将其序列化为字节流之后持久化到数据库中，kafka发送的时候, 消息内容是序列化之后的二进制byte数组，消费端在接收到消息时，需要进行反序列化和解码后，才能得到消息对象。因此我们设计的api如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">@KafkaConsumer(groupId = &quot;TEST01&quot;, topic =&quot;GOODS&quot;)</span><br><span class="line">public class EventConsumer &#123;</span><br><span class="line">   //定义消息的解码器</span><br><span class="line">    @KafkaListener(serializer = GoodsCreatedEventSerializer.class)</span><br><span class="line">    public void onGoodsCreatedEvent(GoodsCreatedEvent event) &#123;</span><br><span class="line">        // dosomething</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @KafkaListener(serializer = GoodsRegisteredEventSerializer.class)</span><br><span class="line">    public void onGoodsRegisteredEvent(GoodsRegisteredEvent event) &#123;</span><br><span class="line">        // dosomething</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@KafkaConsumer(groupId = &quot;TEST01&quot;, topic =&quot;SUPPLIER&quot;)</span><br><span class="line">public class EventConsumer &#123;</span><br><span class="line">   </span><br><span class="line">    @KafkaListener(serializer = SupplierCreatedEventSerializer.class)</span><br><span class="line">    public void onSupplierCreatedEvent(SupplierCreatedEvent event) &#123;</span><br><span class="line">        // dosomething</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>@KafkaConsumer</code>标于监听类上，定义消费者组和关心的topic。<br><br><code>@KafkaListener</code>标于监听方法上,定义该方法监听的消息类型所需要的<strong>序列化器的class</strong>。</p>
<h3 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h3><ul>
<li>业务系统需要把如上两个消息监听类交给Spring进行管理。</li>
<li>在启动容器时，eventbus类库里的<code>MsgAnnotationBeanPostProcessor</code>会对spring管理的bean进行扫描，如果发现有bean类上面有<code>@KafkaConsumer</code>注解时，就会将进一步判断该类下面是否有标有<code>@KafkaListener</code>的方法，代码如下（只显示主要逻辑）</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 实例化及依赖注入完成后、在任何初始化代码（比如配置文件中的init-method）调用之后调用</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">postProcessAfterInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException </span>&#123;</span><br><span class="line">    Class&lt;?&gt; targetClass = AopUtils.getTargetClass(bean);</span><br><span class="line">    <span class="comment">//获取类上是否有注解 @KafkaConsumer</span></span><br><span class="line">    Optional&lt;KafkaConsumer&gt; kafkaConsumer = findListenerAnnotations(targetClass);</span><br><span class="line">    <span class="comment">//类上是否有注解</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">boolean</span> hasKafkaConsumer = kafkaConsumer.isPresent();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (hasKafkaConsumer) &#123;</span><br><span class="line">        <span class="comment">//方法列表 ，查找方法上标有 @KafkaListener 的注解</span></span><br><span class="line">        Map&lt;Method, Set&lt;KafkaListener&gt;&gt; annotatedMethods = MethodIntrospector.selectMethods(targetClass,</span><br><span class="line">            (MethodIntrospector.MetadataLookup&lt;Set&lt;KafkaListener&gt;&gt;) method -&gt; &#123;</span><br><span class="line">                Set&lt;KafkaListener&gt; listenerMethods = findListenerAnnotations(method);</span><br><span class="line">                <span class="keyword">return</span> (!listenerMethods.isEmpty() ? listenerMethods : <span class="keyword">null</span>);</span><br><span class="line">            &#125;);</span><br><span class="line">        <span class="keyword">if</span> (annotatedMethods.isEmpty()) &#123;</span><br><span class="line">            <span class="comment">// throw Exception</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!annotatedMethods.isEmpty()) &#123;</span><br><span class="line">            <span class="comment">// Non-empty set of methods</span></span><br><span class="line">            <span class="keyword">for</span> (Map.Entry&lt;Method, Set&lt;KafkaListener&gt;&gt; entry : annotatedMethods.entrySet()) &#123;</span><br><span class="line">                Method method = entry.getKey();</span><br><span class="line">                <span class="keyword">for</span> (KafkaListener listener : entry.getValue()) &#123;</span><br><span class="line">                    <span class="comment">// process annotation information</span></span><br><span class="line">                    processKafkaListener(kafkaConsumer.get(), listener, method, bean, beanName);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            logger.info(<span class="string">"there are &#123;&#125; methods have @KafkaListener on This bean "</span>, binlogMethods.size());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.logger.info(<span class="string">"No @KafkaConsumer annotations found on bean type: "</span> + bean.getClass());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> bean;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>如果当前bean有<code>@KafkaConsumer</code>注解和<code>@KafkaListener</code>注解，就会创建当前<code>consumer</code>上下文，保存该类的<code>groupId,topic,serializer</code>等信息，并将这些信息注册到<code>KafkaListenerRegistrar</code>上面，KafkaListenerRegistrar会以groupId和topic联合作为key，并创建一个<code>kafkaConsumer</code>实例，放入一个实例列表中，相同的groupId和topic会使用同一个kafkaConsumer实例。代码如下：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaListenerRegistrar</span> <span class="keyword">implements</span> <span class="title">InitializingBean</span> </span>&#123;</span><br><span class="line">    <span class="comment">//kafkaConsumer实例列表</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;String, MsgKafkaConsumer&gt; EVENT_CONSUMERS = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//注册业务consumer</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registerEndpoint</span><span class="params">(ConsumerEndpoint endpoint)</span> </span>&#123;</span><br><span class="line">        Assert.notNull(endpoint, <span class="string">"Endpoint must be set"</span>);</span><br><span class="line">        <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">            addConsumer(endpoint);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//添加consumer到列表，根据key判断</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addConsumer</span><span class="params">(ConsumerEndpoint endpoint)</span> </span>&#123;</span><br><span class="line">        String groupId = endpoint.getGroupId();</span><br><span class="line">        String topic = endpoint.getTopic();</span><br><span class="line">        String kafkaHost = endpoint.getKafkaHost();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 默认 group id</span></span><br><span class="line">            String className = endpoint.getBean().getClass().getName();</span><br><span class="line">            groupId = <span class="string">""</span>.equals(groupId) ? className : groupId;</span><br><span class="line">            String consumerKey = groupId + <span class="string">":"</span> +   topic;</span><br><span class="line">            <span class="comment">//判断key</span></span><br><span class="line">            <span class="keyword">if</span> (EVENT_CONSUMERS.containsKey(consumerKey)) &#123;</span><br><span class="line">                EVENT_CONSUMERS.get(consumerKey).addConsumer(endpoint);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                MsgKafkaConsumer consumer = <span class="keyword">new</span> MsgKafkaConsumer(kafkaHost, groupId, topic);</span><br><span class="line">                consumer.addConsumer(endpoint);</span><br><span class="line">                EVENT_CONSUMERS.put(consumerKey, consumer);</span><br><span class="line">            &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            logger.error(e.getMessage(), e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//当所有bean被扫描完后，会调用此方法，启动kafkaConsumer实例</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterPropertiesSet</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        logger.info(<span class="string">"ready to start consumer ,event consumer size &#123;&#125;, binlog consumer size &#123;&#125;"</span>, EVENT_CONSUMERS.size(), BINLOG_CONSUMERS.size());</span><br><span class="line"></span><br><span class="line">        EVENT_CONSUMERS.values().forEach(Thread::start);</span><br><span class="line"></span><br><span class="line">        BINLOG_CONSUMERS.values().forEach(Thread::start);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>在上面代码中，MsgKafkaConsumer处理消息接收和解码，并将解码后生产的具体事件内容作为参数，通过反射调用关注了该topic的方法，主要逻辑如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"> public void run() &#123;</span><br><span class="line">        logger.info(&quot;[KafkaConsumer][&#123;&#125;][run] &quot;, this.groupId + &quot;:&quot; + this.topic);</span><br><span class="line">        this.consumer.subscribe(Arrays.asList(this.topic));</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                ConsumerRecords&lt;Long, byte[]&gt; records = consumer.poll(100);</span><br><span class="line">                for (ConsumerRecord&lt;Long, byte[]&gt; record : records) &#123;</span><br><span class="line">                    logger.info(&quot;receive message,ready to process, topic: &#123;&#125; ,partition: &#123;&#125; ,offset: &#123;&#125;&quot;,</span><br><span class="line">                            record.topic(), record.partition(), record.offset());</span><br><span class="line"></span><br><span class="line">                    for (ConsumerEndpoint consumer : bizConsumers) &#123;</span><br><span class="line">                        dealMessage(consumer, record.value());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                try &#123;</span><br><span class="line">consumer.commitSync();</span><br><span class="line">                &#125; catch (CommitFailedException e) &#123;</span><br><span class="line">                    logger.error(&quot;commit failed&quot;, e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                logger.error(&quot;[KafkaConsumer][&#123;&#125;][run] &quot; + e.getMessage(), groupId + &quot;:&quot; + topic, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 收到消息后具体处理</span><br><span class="line">    protected void dealMessage(ConsumerEndpoint consumer, byte[] message) &#123;</span><br><span class="line">        logger.info(&quot;Iterator and process biz message groupId: &#123;&#125;, topic: &#123;&#125;&quot;, groupId, topic);</span><br><span class="line"></span><br><span class="line">        KafkaMessageProcessor processor = new KafkaMessageProcessor();</span><br><span class="line">        String eventType = processor.getEventType(message);</span><br><span class="line"></span><br><span class="line">        List&lt;Class&lt;?&gt;&gt; parameterTypes = consumer.getParameterTypes();</span><br><span class="line"></span><br><span class="line">        long count = parameterTypes.stream()</span><br><span class="line">                .filter(param -&gt; param.getName().equals(eventType))</span><br><span class="line">                .count();</span><br><span class="line"></span><br><span class="line">        if (count &gt; 0) &#123;</span><br><span class="line">            byte[] eventBinary = processor.getEventBinary();</span><br><span class="line"></span><br><span class="line">            try &#123;</span><br><span class="line">                Object event = processor.decodeMessage(eventBinary, consumer.getEventSerializer());</span><br><span class="line">                consumer.getMethod().invoke(consumer.getBean(), event);</span><br><span class="line">                logger.info(&quot;invoke message end ,bean: &#123;&#125;, method: &#123;&#125;&quot;, consumer.getBean(), consumer.getMethod());</span><br><span class="line">            &#125; catch (IllegalAccessException | InvocationTargetException | IllegalArgumentException e) &#123;</span><br><span class="line">                logger.error(&quot;参数不合法，当前方法虽然订阅此topic，但是不接收当前事件:&quot; + eventType, e);</span><br><span class="line">            &#125; catch (TException e) &#123;</span><br><span class="line">                logger.error(e.getMessage(), e);</span><br><span class="line">                logger.error(&quot;反序列化事件&quot; + eventType + &quot;出错&quot;);</span><br><span class="line">            &#125; catch (InstantiationException e) &#123;</span><br><span class="line">                logger.error(e.getMessage(), e);</span><br><span class="line">                logger.error(&quot;实例化事件&quot; + eventType + &quot;对应的编解码器失败&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            logger.debug(&quot;方法 [ &#123;&#125; ] 不接收当前收到的消息类型 &#123;&#125; &quot;, consumer.getMethod(), eventType);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>消费者设置为手动提交偏移量，在一次poll请求中，当消息成功消费后，会同步提交本次poll的消息偏移量。</p>
<p>如果消息消费了，但是在未提交偏移量之前，消息是不会丢失的（未处理）。当消息消费后，准备提交偏移量是系统出错了，这时候会导致重复消费，因此也是至少一次的语义。所以这里交给业务系统时，还是需要业务去实现业务去重。</p>
<h3 id="消费端注意事项"><a href="#消费端注意事项" class="headerlink" title="消费端注意事项"></a>消费端注意事项</h3><p>我们在topic的基础上进行了更细粒化的区分，消息能够根据订阅者所感兴趣的事件类型进行选择性发送。<br></p>
<p>即使两个订阅者方法都订阅了同一个topic，kafka消费者仍然会根据接收到的消息的类型和订阅者方法的所感兴趣的事件类型进行对比，如果当前接收到的事件与订阅事件一致，便会通过反射<strong>调用订阅者方法</strong>，并将收到的具体事件也传递过去，事件接收成功。</p>
<p>举个例子：</p>
<p>如果<code>kafkaConsumer</code>从broker接收到消息，解码之后发现事件类型为<code>UserCreated</code>,那他只会notify接收参数为<code>UserCreated</code>事件的方法，而不会去调用接收<code>UserRegisted</code>的订阅者方法。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>DDD领域事件的消息总线分析就到这里，本文重点讲了跨领域时，通过kafka消息中间件如何做到消息的稳定发送和接收，不出现消息丢失，触发事件而未发送等情况。</p>
</div><div class="tags"><a href="/tags/DDD/">DDD</a><a href="/tags/领域驱动设计/">领域驱动设计</a><a href="/tags/领域事件/">领域事件</a><a href="/tags/eventbus/">eventbus</a><a href="/tags/dapeng/">dapeng</a></div><div class="post-nav"><a class="pre" href="/2018/04/30/influxdb-commons-action/">influxdb常用操作</a><a class="next" href="/2018/03/10/DDD-event-bus/">领域事件及事件总线EventBus使用实践</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://git66.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/记录/" style="font-size: 15px;">记录</a> <a href="/tags/Angular/" style="font-size: 15px;">Angular</a> <a href="/tags/Arcgis-for-JavaScript/" style="font-size: 15px;">Arcgis for JavaScript</a> <a href="/tags/地图/" style="font-size: 15px;">地图</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/github-pages/" style="font-size: 15px;">github-pages</a> <a href="/tags/blog/" style="font-size: 15px;">blog</a> <a href="/tags/shell/" style="font-size: 15px;">shell</a> <a href="/tags/influxdb/" style="font-size: 15px;">influxdb</a> <a href="/tags/时序数据库/" style="font-size: 15px;">时序数据库</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/函数式编程/" style="font-size: 15px;">函数式编程</a> <a href="/tags/vue-js/" style="font-size: 15px;">vue.js</a> <a href="/tags/WebGis/" style="font-size: 15px;">WebGis</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/电影/" style="font-size: 15px;">电影</a> <a href="/tags/DDD/" style="font-size: 15px;">DDD</a> <a href="/tags/领域驱动设计/" style="font-size: 15px;">领域驱动设计</a> <a href="/tags/领域事件/" style="font-size: 15px;">领域事件</a> <a href="/tags/eventbus/" style="font-size: 15px;">eventbus</a> <a href="/tags/dapeng/" style="font-size: 15px;">dapeng</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/版本控制/" style="font-size: 15px;">版本控制</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/ubuntu/" style="font-size: 15px;">ubuntu</a> <a href="/tags/笔记/" style="font-size: 15px;">笔记</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/07/15/vue-life/">vue实例生命周期</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/26/maven-plugins-use/">maven常用插件使用范例</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/28/wu-wen-xi-dong/">无问西东</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/28/tscompose-bai-hua/">白话tscompose</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/30/influxdb-commons-action/">influxdb常用操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/16/eventbus-archtech-0316/">事件总线EventBus实现架构原理分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/10/DDD-event-bus/">领域事件及事件总线EventBus使用实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/06/scala-function-5w/">scala函数的演变过程,函数式的优点</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/16/Angular-load-ArcGis-for-JavaScript/">Angular(4)中加载Arcgis for JavaScript地图</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/09/hexo-all-update-by-shell/">一个命令,解决hexo多机更新操作</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.itjava.top" title="枫叶博客" target="_blank">枫叶博客</a><ul></ul><a href="https://www.teng.im" title="天山揽月" target="_blank">天山揽月</a><ul></ul><a href="https://www.legic.xyz" title="Legend魔法馆" target="_blank">Legend魔法馆</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">struy.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>